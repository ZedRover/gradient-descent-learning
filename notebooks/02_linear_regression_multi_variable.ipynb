{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Linear Regression (Multiple Variables) - Matrix Form\n\n## What's Different from Single Variable?\n\nIn the previous notebook, we predicted house prices using just **one feature** (size). But real-world problems usually have **multiple features**!\n\nFor example, predicting house prices using:\n- Size (sq ft)\n- Number of bedrooms\n- Age of house\n- Distance to city center\n\n### The Equation\n\n**Single variable:** $y = mx + b$\n\n**Multiple variables:** $y = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$\n\nWhere:\n- $w_1, w_2, w_3, w_4$ are **weights** (like the slope $m$, but one for each feature)\n- $x_1, x_2, x_3, x_4$ are our **features** (size, bedrooms, age, distance)\n- $b$ is the **bias** (like the intercept)\n\n## Matrix Notation (Don't Panic!)\n\nInstead of writing $w_1x_1 + w_2x_2 + ...$, we use **matrices** to make it cleaner:\n\n$$\\boxed{y = Xw + b}$$\n\nWhere:\n- $X$ is a matrix of all features (rows = samples, columns = features)\n- $w$ is a vector of all weights\n- $b$ is a scalar (single number)\n\n### Why matrices?\n1. Cleaner notation\n2. Faster computation (NumPy is optimized for matrix operations)\n3. Works for any number of features\n\nThink of matrix multiplication as: **\"for each sample, multiply each feature by its weight and sum them up\"**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Generate Sample Data\n\nLet's create data with 3 features:\n- Feature 1: House size (sq ft)\n- Feature 2: Number of bedrooms\n- Feature 3: House age (years)\n\n$$\\text{Price} = 50 \\times \\text{size} + 20 \\times \\text{bedrooms} - 5 \\times \\text{age} + 100 + \\text{noise}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "n_samples = 200\n",
    "n_features = 3\n",
    "\n",
    "# Features: [size, bedrooms, age]\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "X[:, 0] = X[:, 0] * 20 + 50  # Size: mean=50, std=20\n",
    "X[:, 1] = np.abs(X[:, 1] * 1 + 3).astype(int)  # Bedrooms: around 3\n",
    "X[:, 2] = np.abs(X[:, 2] * 10 + 15)  # Age: around 15 years\n",
    "\n",
    "# True weights and bias\n",
    "true_w = np.array([50, 20, -5])  # [size_weight, bedroom_weight, age_weight]\n",
    "true_b = 100\n",
    "\n",
    "# Generate target with noise\n",
    "noise = np.random.normal(0, 200, n_samples)\n",
    "y = X @ true_w + true_b + noise  # @ is matrix multiplication\n",
    "\n",
    "print(\"Data Shape:\")\n",
    "print(f\"X: {X.shape} (samples × features)\")\n",
    "print(f\"y: {y.shape} (samples)\")\n",
    "print(f\"\\nTrue weights: {true_w}\")\n",
    "print(f\"True bias: {true_b}\")\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(f\"Size (sq ft):  mean={X[:, 0].mean():.2f}, std={X[:, 0].std():.2f}\")\n",
    "print(f\"Bedrooms:      mean={X[:, 1].mean():.2f}, std={X[:, 1].std():.2f}\")\n",
    "print(f\"Age (years):   mean={X[:, 2].mean():.2f}, std={X[:, 2].std():.2f}\")\n",
    "print(f\"Price:         mean={y.mean():.2f}, std={y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize the Data\n",
    "\n",
    "With multiple features, we can't plot everything in 2D anymore. Let's look at pairwise relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "feature_names = ['Size (sq ft)', 'Bedrooms', 'Age (years)']\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].scatter(X[:, i], y, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "    axes[i].set_xlabel(feature_names[i], fontsize=12)\n",
    "    axes[i].set_ylabel('Price ($1000s)', fontsize=12)\n",
    "    axes[i].set_title(f'{feature_names[i]} vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Understanding the Math\n\n### Prediction Formula\n\nFor a single sample:\n- $\\hat{y} = w_1x_1 + w_2x_2 + w_3x_3 + b$\n\nFor all samples at once (vectorized):\n- $\\hat{y} = Xw + b$\n\nWhere:\n- $X$ shape: $(n_{samples}, n_{features})$\n- $w$ shape: $(n_{features},)$\n- Result shape: $(n_{samples},)$\n\n### Mean Squared Error (MSE)\n\n$$L = \\frac{1}{n} \\sum (y - \\hat{y})^2 = \\frac{1}{n} ||y - Xw - b||^2$$\n\n### Computing Gradients\n\nWe need partial derivatives for **each weight** and the **bias**.\n\n#### For weights $\\left(\\frac{\\partial L}{\\partial w}\\right)$:\n\nLet's derive this step by step:\n\n1. $$L = \\frac{1}{n} \\sum_i \\left(y_i - \\left(\\sum_j w_j x_{ij} + b\\right)\\right)^2$$\n\n2. For a specific weight $w_k$:\n   $$\\frac{\\partial L}{\\partial w_k} = \\frac{1}{n} \\sum_i \\left(2(y_i - \\hat{y}_i) \\times \\frac{\\partial \\hat{y}_i}{\\partial w_k}\\right)$$\n   \n3. Since $\\hat{y}_i = \\sum_j w_j x_{ij} + b$:\n   $$\\frac{\\partial \\hat{y}_i}{\\partial w_k} = x_{ik} \\text{ (the k-th feature of sample i)}$$\n\n4. Therefore:\n   $$\\boxed{\\frac{\\partial L}{\\partial w_k} = \\frac{-2}{n} \\sum_i (y_i - \\hat{y}_i) \\times x_{ik}}$$\n\nIn matrix form:\n$$\\boxed{\\frac{\\partial L}{\\partial w} = \\frac{-2}{n} X^T(y - \\hat{y})}$$\n\nWhere $X^T$ is the transpose of $X$ (swap rows and columns).\n\n#### For bias $\\left(\\frac{\\partial L}{\\partial b}\\right)$:\n\nSame as before:\n$$\\boxed{\\frac{\\partial L}{\\partial b} = \\frac{-2}{n} \\sum (y - \\hat{y})}$$\n\n### What does this mean?\n\n- $\\frac{\\partial L}{\\partial w_k}$: For each weight, we look at how that feature correlates with the error\n  - If errors are positive when feature $x_k$ is large, we should decrease $w_k$\n  - If errors are negative when feature $x_k$ is large, we should increase $w_k$\n\n- $X^T(y - \\hat{y})$: This efficiently computes all weight gradients at once!\n  - Each element is the sum of (error $\\times$ feature) for that feature\n  - Matrix multiplication does this automatically"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMultiVar:\n",
    "    \"\"\"Linear Regression with Multiple Variables using Gradient Descent\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None  # weights\n",
    "        self.b = None  # bias\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "        n = len(y)\n",
    "        predictions = X @ self.w + self.b  # Matrix multiplication\n",
    "        mse = (1/n) * np.sum((y - predictions) ** 2)\n",
    "        return mse\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Calculate partial derivatives for all weights and bias\"\"\"\n",
    "        n = len(y)\n",
    "        predictions = X @ self.w + self.b\n",
    "        errors = y - predictions\n",
    "        \n",
    "        # Gradient for weights (vectorized!)\n",
    "        # Shape: (n_features,)\n",
    "        dw = (-2/n) * (X.T @ errors)\n",
    "        \n",
    "        # Gradient for bias\n",
    "        db = (-2/n) * np.sum(errors)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.random.randn(n_features) * 0.01\n",
    "        self.b = 0.0\n",
    "        \n",
    "        print(f\"Initial weights: {self.w}\")\n",
    "        print(f\"Initial bias: {self.b:.4f}\\n\")\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Calculate gradients\n",
    "            dw, db = self.compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w = self.w - self.lr * dw\n",
    "            self.b = self.b - self.lr * db\n",
    "            \n",
    "            # Track cost\n",
    "            cost = self.compute_cost(X, y)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (i % 100 == 0 or i == self.n_iterations - 1):\n",
    "                print(f\"Iteration {i:4d} | Cost: {cost:12.2f} | weights: {self.w} | bias: {self.b:8.4f}\")\n",
    "        \n",
    "        print(f\"\\nFinal weights: {self.w}\")\n",
    "        print(f\"True weights:  {true_w}\")\n",
    "        print(f\"\\nFinal bias: {self.b:.4f}\")\n",
    "        print(f\"True bias:  {true_b:.4f}\")\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Feature Normalization (Important!)\n\n### Why normalize?\n\nOur features have very different scales:\n- Size: 10-100\n- Bedrooms: 1-5\n- Age: 0-30\n\nThis causes problems:\n1. Gradients for different features have different magnitudes\n2. One learning rate doesn't work well for all features\n3. Convergence is slower\n\n### Solution: Standardization\n\nTransform each feature to have:\n- Mean = 0\n- Standard deviation = 1\n\n**Formula:** $$x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}$$\n\nwhere $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n\nThis puts all features on the same scale!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X_mean = X.mean(axis=0)\n",
    "X_std = X.std(axis=0)\n",
    "X_normalized = (X - X_mean) / X_std\n",
    "\n",
    "print(\"Before normalization:\")\n",
    "print(f\"Mean: {X.mean(axis=0)}\")\n",
    "print(f\"Std:  {X.std(axis=0)}\")\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"Mean: {X_normalized.mean(axis=0)}\")\n",
    "print(f\"Std:  {X_normalized.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with normalized features\n",
    "model = LinearRegressionMultiVar(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_normalized)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Cost history\n",
    "axes[0, 0].plot(model.cost_history, linewidth=2, color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Cost (MSE)', fontsize=12)\n",
    "axes[0, 0].set_title('Learning Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predictions vs Actual\n",
    "axes[0, 1].scatter(y, y_pred, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Price', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Predicted Price', fontsize=12)\n",
    "axes[0, 1].set_title('Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "residuals = y - y_pred\n",
    "axes[1, 0].scatter(y_pred, residuals, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Price', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[1, 0].set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Weight comparison\n",
    "x_pos = np.arange(len(model.w))\n",
    "width = 0.35\n",
    "axes[1, 1].bar(x_pos - width/2, model.w, width, label='Learned Weights', alpha=0.8)\n",
    "# Note: true_w needs to be adjusted for normalized data\n",
    "true_w_normalized = true_w * X_std  # Adjust for normalization\n",
    "axes[1, 1].bar(x_pos + width/2, true_w_normalized, width, label='True Weights (adjusted)', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Feature Index', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Weight Value', fontsize=12)\n",
    "axes[1, 1].set_title('Learned vs True Weights', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(['Size', 'Bedrooms', 'Age'])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "mse = np.mean((y - y_pred) ** 2)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - y.mean()) ** 2))\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"MSE:  {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Understanding Weight Magnitudes\n",
    "\n",
    "Let's interpret what our learned weights mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert normalized weights back to original scale\n",
    "w_original_scale = model.w / X_std\n",
    "b_original_scale = model.b - np.sum(w_original_scale * X_mean)\n",
    "\n",
    "print(\"Weights in Original Scale:\")\n",
    "print(f\"Size weight:     {w_original_scale[0]:8.4f} (true: {true_w[0]})\")\n",
    "print(f\"Bedroom weight:  {w_original_scale[1]:8.4f} (true: {true_w[1]})\")\n",
    "print(f\"Age weight:      {w_original_scale[2]:8.4f} (true: {true_w[2]})\")\n",
    "print(f\"Bias:            {b_original_scale:8.4f} (true: {true_b})\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- Each additional sq ft increases price by ${w_original_scale[0]:.2f}k\")\n",
    "print(f\"- Each additional bedroom increases price by ${w_original_scale[1]:.2f}k\")\n",
    "print(f\"- Each additional year of age changes price by ${w_original_scale[2]:.2f}k\")\n",
    "print(f\"- Base price (intercept) is ${b_original_scale:.2f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize 3D Decision Boundary (Using First 2 Features)\n",
    "\n",
    "Let's visualize the relationship between 2 features and the target in 3D!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot using first 2 features\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Size vs Price (with bedroom color coding)\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "scatter = ax1.scatter(X[:, 0], X[:, 1], y, c=y, cmap='viridis', \n",
    "                      alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Size (sq ft)', fontsize=10)\n",
    "ax1.set_ylabel('Bedrooms', fontsize=10)\n",
    "ax1.set_zlabel('Price ($1000s)', fontsize=10)\n",
    "ax1.set_title('3D Visualization: Size × Bedrooms → Price', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax1, label='Price', shrink=0.5)\n",
    "\n",
    "# Plot 2: Create a mesh for the regression plane\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "# Create mesh\n",
    "x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)\n",
    "x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)\n",
    "x1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Use mean age for the 3rd feature\n",
    "x3_mean = X[:, 2].mean()\n",
    "X_mesh = np.column_stack([\n",
    "    x1_mesh.ravel(),\n",
    "    x2_mesh.ravel(),\n",
    "    np.full(x1_mesh.size, x3_mean)\n",
    "])\n",
    "X_mesh_normalized = (X_mesh - X_mean) / X_std\n",
    "y_mesh = model.predict(X_mesh_normalized).reshape(x1_mesh.shape)\n",
    "\n",
    "# Plot surface\n",
    "ax2.plot_surface(x1_mesh, x2_mesh, y_mesh, alpha=0.3, cmap='viridis')\n",
    "ax2.scatter(X[:, 0], X[:, 1], y, c='red', alpha=0.5, s=30)\n",
    "ax2.set_xlabel('Size (sq ft)', fontsize=10)\n",
    "ax2.set_ylabel('Bedrooms', fontsize=10)\n",
    "ax2.set_zlabel('Price ($1000s)', fontsize=10)\n",
    "ax2.set_title('Regression Plane (Age=mean)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### What We Learned:\n\n1. **Multi-variable linear regression** extends single-variable to handle multiple features\n   - Instead of: $y = mx + b$\n   - We have: $y = w_1x_1 + w_2x_2 + \\ldots + w_nx_n + b$\n\n2. **Matrix notation** makes the math cleaner and computation faster\n   - $y = Xw + b$\n   - $X$ is $(n_{samples}, n_{features})$\n   - $w$ is $(n_{features},)$\n   - One matrix multiplication instead of many scalar multiplications!\n\n3. **Vectorized gradients** compute all weight updates at once\n   - $$\\boxed{\\frac{\\partial L}{\\partial w} = \\frac{-2}{n} X^T(y - \\hat{y})}$$\n   - $X^T \\cdot \\text{errors}$ computes weighted sum of errors for each feature\n   - Much faster than computing each weight gradient separately\n\n4. **Feature normalization** is crucial\n   - Different feature scales → different gradient magnitudes\n   - Standardization (mean=0, std=1) fixes this\n   - Faster convergence, one learning rate works for all features\n\n5. **Matrix transpose** $(X^T)$:\n   - Swaps rows and columns\n   - Shape $(n_{samples}, n_{features}) \\rightarrow (n_{features}, n_{samples})$\n   - $X^T \\cdot \\text{errors}$: for each feature, sum up (error $\\times$ feature_value)\n\n### The Gradient Formula Explained:\n\n$$\\boxed{\\frac{\\partial L}{\\partial w} = \\frac{-2}{n} X^T(y - \\hat{y})}$$\n\nBreaking it down:\n- $(y - \\hat{y})$: Vector of errors for all samples\n- $X^T$: Transpose of feature matrix\n- $X^T(y - \\hat{y})$: For each feature, compute sum of (error $\\times$ feature_value)\n- $\\frac{-2}{n}$: Scaling factor (from derivative of squared error)\n\nThink of it as:\n\"For each weight, how much does that feature correlate with the errors?\"\n- High positive correlation → increase weight\n- High negative correlation → decrease weight\n- No correlation → don't change weight\n\n### Why This Matters for Neural Networks:\n\nThis is the foundation for deep learning!\n- Neural networks are many linear regressions stacked together\n- Each layer: $z = Xw + b$\n- Backpropagation uses the same chain rule ideas\n- Matrix operations scale to millions of parameters"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradient-descent-hr-claude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}