{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Logistic Regression - Classification with Gradient Descent\n\n## What's Different from Linear Regression?\n\n**Linear Regression**: Predicts continuous values (e.g., house prices: $200k, $350k, etc.)\n\n**Logistic Regression**: Predicts discrete classes (e.g., spam or not spam: 0 or 1)\n\n### The Problem\n\nImagine we want to predict if a student will pass (1) or fail (0) based on hours studied.\n\nIf we use linear regression:\n- Prediction could be 1.5 (what does that mean?)\n- Or -0.3 (negative probability?)\n\n**We need predictions between 0 and 1 (like probabilities)!**\n\n## The Solution: Sigmoid Function\n\nThe **sigmoid function** squashes any number to a value between 0 and 1.\n\n**Formula:** $$\\boxed{\\sigma(z) = \\frac{1}{1 + e^{-z}}}$$\n\nWhere:\n- $z = wx + b$ (same as linear regression)\n- $\\sigma(z)$ = probability of being class 1\n\nProperties:\n- When $z \\to +\\infty$, $\\sigma(z) \\to 1$\n- When $z \\to -\\infty$, $\\sigma(z) \\to 0$\n- When $z = 0$, $\\sigma(z) = 0.5$\n- S-shaped curve (smooth transition from 0 to 1)\n\n### The Model\n\n$$\\boxed{\\hat{y} = \\sigma(Xw + b) = \\frac{1}{1 + e^{-(Xw + b)}}}$$\n\n**Decision rule:**\n- If $\\hat{y} \\geq 0.5$ → predict class 1\n- If $\\hat{y} < 0.5$ → predict class 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understanding the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Create range of values\n",
    "z = np.linspace(-10, 10, 100)\n",
    "y_sigmoid = sigmoid(z)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left: Sigmoid function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, y_sigmoid, linewidth=3, color='#2E86AB')\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Decision Boundary (0.5)')\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=1, alpha=0.3)\n",
    "plt.axhline(y=1, color='gray', linestyle='-', linewidth=1, alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='-', linewidth=1, alpha=0.3)\n",
    "plt.xlabel('z = wx + b', fontsize=12)\n",
    "plt.ylabel('σ(z)', fontsize=12)\n",
    "plt.title('Sigmoid Function: σ(z) = 1/(1 + e⁻ᶻ)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Right: Derivative of sigmoid\n",
    "plt.subplot(1, 2, 2)\n",
    "# Derivative: σ'(z) = σ(z) * (1 - σ(z))\n",
    "y_sigmoid_derivative = y_sigmoid * (1 - y_sigmoid)\n",
    "plt.plot(z, y_sigmoid_derivative, linewidth=3, color='#A23B72')\n",
    "plt.axvline(x=0, color='gray', linestyle='-', linewidth=1, alpha=0.3)\n",
    "plt.xlabel('z = wx + b', fontsize=12)\n",
    "plt.ylabel(\"σ'(z)\", fontsize=12)\n",
    "plt.title('Derivative of Sigmoid: σ\\'(z) = σ(z)(1 - σ(z))', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Values:\")\n",
    "print(f\"σ(-10) = {sigmoid(-10):.6f} ≈ 0\")\n",
    "print(f\"σ(-5)  = {sigmoid(-5):.6f}\")\n",
    "print(f\"σ(0)   = {sigmoid(0):.6f} = 0.5\")\n",
    "print(f\"σ(5)   = {sigmoid(5):.6f}\")\n",
    "print(f\"σ(10)  = {sigmoid(10):.6f} ≈ 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Binary Cross-Entropy Loss\n\n### Why not use Mean Squared Error (MSE)?\n\nFor classification, MSE doesn't work well:\n- Not convex with sigmoid (multiple local minima)\n- Gradients can become very small (vanishing gradients)\n\n### Binary Cross-Entropy (Log Loss)\n\n**Formula:** $$\\boxed{L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\\right]}$$\n\nWhere:\n- $y$ = actual label (0 or 1)\n- $\\hat{y}$ = predicted probability\n\n### Why does this work?\n\nLet's break it down for a single sample:\n\n**If $y = 1$** (actual class is 1):\n- Loss $= -\\log(\\hat{y})$\n- If $\\hat{y} = 1$ (confident correct prediction) → Loss $= 0$\n- If $\\hat{y} = 0.5$ (uncertain) → Loss $= 0.69$\n- If $\\hat{y} \\approx 0$ (confident wrong prediction) → Loss $\\to \\infty$\n\n**If $y = 0$** (actual class is 0):\n- Loss $= -\\log(1-\\hat{y})$\n- If $\\hat{y} = 0$ (confident correct prediction) → Loss $= 0$\n- If $\\hat{y} = 0.5$ (uncertain) → Loss $= 0.69$\n- If $\\hat{y} \\approx 1$ (confident wrong prediction) → Loss $\\to \\infty$\n\n**Key insight:** We heavily penalize confident wrong predictions!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binary cross-entropy loss\n",
    "y_pred = np.linspace(0.001, 0.999, 100)  # Avoid log(0)\n",
    "\n",
    "# Loss when y=1\n",
    "loss_y1 = -np.log(y_pred)\n",
    "\n",
    "# Loss when y=0\n",
    "loss_y0 = -np.log(1 - y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_pred, loss_y1, linewidth=3, label='Loss when y=1: -log(ŷ)', color='#2E86AB')\n",
    "plt.plot(y_pred, loss_y0, linewidth=3, label='Loss when y=0: -log(1-ŷ)', color='#A23B72')\n",
    "plt.xlabel('Predicted Probability (ŷ)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Binary Cross-Entropy Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss Examples:\")\n",
    "print(\"\\nWhen actual y=1:\")\n",
    "print(f\"  Predict 0.99 → Loss = {-np.log(0.99):.4f}\")\n",
    "print(f\"  Predict 0.50 → Loss = {-np.log(0.50):.4f}\")\n",
    "print(f\"  Predict 0.01 → Loss = {-np.log(0.01):.4f}\")\n",
    "print(\"\\nWhen actual y=0:\")\n",
    "print(f\"  Predict 0.01 → Loss = {-np.log(0.99):.4f}\")\n",
    "print(f\"  Predict 0.50 → Loss = {-np.log(0.50):.4f}\")\n",
    "print(f\"  Predict 0.99 → Loss = {-np.log(0.01):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Deriving the Gradients\n\nThis is where the magic happens! Let's derive the gradients step by step.\n\n### Setup\n\n- $z = Xw + b$ (linear combination)\n- $\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$ (prediction)\n- $L = -\\frac{1}{n} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$ (loss)\n\n### Gradient with respect to $w$\n\nWe need: $\\frac{\\partial L}{\\partial w}$\n\nUsing chain rule:\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$$\n\nLet's compute each part:\n\n#### Part 1: $\\frac{\\partial L}{\\partial \\hat{y}}$\n\n$$L = -\\frac{1}{n} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n\nFor a single sample:\n$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\left(\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}\\right) = -\\frac{y - \\hat{y}}{\\hat{y}(1-\\hat{y})}$$\n\n#### Part 2: $\\frac{\\partial \\hat{y}}{\\partial z}$ (Derivative of Sigmoid)\n\n$$\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\nUsing quotient rule:\n- Let $u = 1$, $v = 1 + e^{-z}$\n- $\\hat{y} = \\frac{u}{v}$\n- $\\frac{\\partial \\hat{y}}{\\partial z} = \\frac{v \\cdot \\frac{\\partial u}{\\partial z} - u \\cdot \\frac{\\partial v}{\\partial z}}{v^2}$\n- $\\frac{\\partial v}{\\partial z} = e^{-z} \\times (-1) = -e^{-z}$\n- $\\frac{\\partial \\hat{y}}{\\partial z} = \\frac{0 - 1 \\times (-e^{-z})}{(1 + e^{-z})^2} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n- $= \\frac{1}{1 + e^{-z}} \\times \\frac{e^{-z}}{1 + e^{-z}} = \\sigma(z) \\times (1 - \\sigma(z))$\n- $= \\hat{y}(1 - \\hat{y})$\n\n**Beautiful result!** The derivative of sigmoid is $\\sigma(z) \\times (1 - \\sigma(z))$\n\n#### Part 3: $\\frac{\\partial z}{\\partial w}$\n\n$$z = Xw + b$$\n\n$$\\frac{\\partial z}{\\partial w} = X$$\n\n#### Combine Everything\n\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$$\n\n$$= \\left[-\\frac{y - \\hat{y}}{\\hat{y}(1-\\hat{y})}\\right] \\times [\\hat{y}(1-\\hat{y})] \\times X$$\n\n$$= -(y - \\hat{y}) \\times X = (\\hat{y} - y) \\times X$$\n\nFor all samples (vectorized):\n\n$$\\boxed{\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(\\hat{y} - y)}$$\n\n### Gradient with respect to $b$\n\nSimilarly:\n\n$$\\boxed{\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum (\\hat{y} - y)}$$\n\n### Simple Result!\n\nNotice: The gradient formula looks almost the same as linear regression!\n- Linear Regression: $\\frac{\\partial L}{\\partial w} = \\frac{-2}{n} X^T(y - \\hat{y})$\n- Logistic Regression: $\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(\\hat{y} - y)$\n\nThe cross-entropy loss and sigmoid activation simplify beautifully!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_informative=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm = (X_test - X_mean) / X_std\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Class distribution (train): {np.bincount(y_train)}\")\n",
    "print(f\"Class distribution (test):  {np.bincount(y_test)}\")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n",
    "            label='Class 0', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n",
    "            label='Class 1', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Training Data', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], \n",
    "            label='Class 0', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], \n",
    "            label='Class 1', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Test Data', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic Regression using Gradient Descent\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def compute_cost(self, X, y):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        n = len(y)\n",
    "        z = X @ self.w + self.b\n",
    "        y_pred = self.sigmoid(z)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        cost = -(1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def compute_gradients(self, X, y):\n",
    "        \"\"\"Calculate gradients\"\"\"\n",
    "        n = len(y)\n",
    "        z = X @ self.w + self.b\n",
    "        y_pred = self.sigmoid(z)\n",
    "        errors = y_pred - y\n",
    "        \n",
    "        # Gradients\n",
    "        dw = (1/n) * (X.T @ errors)\n",
    "        db = (1/n) * np.sum(errors)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Compute gradients\n",
    "            dw, db = self.compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "            # Track cost\n",
    "            cost = self.compute_cost(X, y)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            if verbose and (i % 100 == 0 or i == self.n_iterations - 1):\n",
    "                print(f\"Iteration {i:4d} | Cost: {cost:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        z = X @ self.w + self.b\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = LogisticRegression(learning_rate=0.1, n_iterations=1000)\n",
    "model.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train_norm)\n",
    "y_test_pred = model.predict(X_test_norm)\n",
    "y_train_proba = model.predict_proba(X_train_norm)\n",
    "y_test_proba = model.predict_proba(X_test_norm)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_train_pred == y_train)\n",
    "test_accuracy = np.mean(y_test_pred == y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Cost history\n",
    "axes[0, 0].plot(model.cost_history, linewidth=2, color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Cost (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[0, 0].set_title('Learning Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "# Create mesh\n",
    "x1_min, x1_max = X_train_norm[:, 0].min() - 1, X_train_norm[:, 0].max() + 1\n",
    "x2_min, x2_max = X_train_norm[:, 1].min() - 1, X_train_norm[:, 1].max() + 1\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
    "                        np.linspace(x2_min, x2_max, 200))\n",
    "X_mesh = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "Z = model.predict_proba(X_mesh).reshape(xx1.shape)\n",
    "\n",
    "# Plot decision boundary\n",
    "contour = axes[0, 1].contourf(xx1, xx2, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[0, 1].contour(xx1, xx2, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[0, 1].scatter(X_train_norm[y_train == 0, 0], X_train_norm[y_train == 0, 1],\n",
    "                   label='Class 0', alpha=0.8, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].scatter(X_train_norm[y_train == 1, 0], X_train_norm[y_train == 1, 1],\n",
    "                   label='Class 1', alpha=0.8, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].set_xlabel('Feature 1 (normalized)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Feature 2 (normalized)', fontsize=12)\n",
    "axes[0, 1].set_title('Decision Boundary (Training Data)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "plt.colorbar(contour, ax=axes[0, 1], label='P(y=1)')\n",
    "\n",
    "# Plot 3: Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Pred 0', 'Pred 1'],\n",
    "            yticklabels=['True 0', 'True 1'])\n",
    "axes[1, 0].set_title('Confusion Matrix (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('True Label', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Plot 4: Probability distribution\n",
    "axes[1, 1].hist(y_test_proba[y_test == 0], bins=20, alpha=0.7, label='Class 0', color='#2E86AB')\n",
    "axes[1, 1].hist(y_test_proba[y_test == 1], bins=20, alpha=0.7, label='Class 1', color='#A23B72')\n",
    "axes[1, 1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1, 1].set_xlabel('Predicted Probability', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Predicted Probability Distribution (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### What We Learned:\n\n1. **Logistic Regression is for Classification**\n   - Linear Regression: predicts continuous values\n   - Logistic Regression: predicts probabilities (0 to 1)\n\n2. **Sigmoid Function** squashes outputs to [0, 1]\n   - $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n   - S-shaped curve\n   - Derivative: $\\sigma'(z) = \\sigma(z) \\times (1 - \\sigma(z))$\n\n3. **Binary Cross-Entropy Loss**\n   - Better than MSE for classification\n   - $L = -\\frac{1}{n} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n   - Heavily penalizes confident wrong predictions\n\n4. **Gradients Simplify Beautifully**\n   - Despite complex loss function, gradient is simple!\n   - $\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T(\\hat{y} - y)$\n   - $\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum (\\hat{y} - y)$\n   - Very similar to linear regression!\n\n5. **The Math Breakdown:**\n\n   **Chain Rule Application:**\n   $$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$$\n   \n   **Each Component:**\n   - $\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y - \\hat{y}}{\\hat{y}(1-\\hat{y})}$\n   - $\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})$\n   - $\\frac{\\partial z}{\\partial w} = X$\n   \n   **Combine (terms cancel!):**\n   $$= \\left[-\\frac{y - \\hat{y}}{\\hat{y}(1-\\hat{y})}\\right] \\times [\\hat{y}(1-\\hat{y})] \\times X = (\\hat{y} - y) \\times X$$\n\n6. **Decision Boundary**\n   - Predict class 1 if $\\sigma(wx + b) \\geq 0.5$\n   - Equivalently: if $wx + b \\geq 0$\n   - Creates a linear boundary in feature space\n\n### Why This Matters:\n\n1. **Foundation for Neural Networks**\n   - Each neuron is basically logistic regression\n   - Stack them → deep learning!\n\n2. **Understanding Activation Functions**\n   - Sigmoid is one type of activation\n   - Others: ReLU, tanh, etc.\n   - All have derivatives for backpropagation\n\n3. **Loss Functions**\n   - Different tasks need different losses\n   - Cross-entropy for classification\n   - MSE for regression\n\n4. **Gradient Descent Still Works!**\n   - Same algorithm as linear regression\n   - Just different loss and activation\n   - This generalizes to neural networks\n\n### Next Steps:\n\nNow we're ready for neural networks!\n- Stack multiple logistic regression units\n- Add hidden layers\n- Backpropagation through multiple layers"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}