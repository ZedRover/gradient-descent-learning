{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Layer Perceptron (1 Hidden Layer) - Introduction to Neural Networks\n\n## From Logistic Regression to Neural Networks\n\nSo far we've learned:\n- **Linear Regression**: $y = Xw + b$ (for continuous predictions)\n- **Logistic Regression**: $y = \\sigma(Xw + b)$ (for binary classification)\n\nBoth are **linear models** - they can only learn linear decision boundaries.\n\n### The Problem with Linear Models\n\nSome problems are **not linearly separable**!\n\nExample: XOR problem\n- Input (0,0) → 0\n- Input (0,1) → 1\n- Input (1,0) → 1\n- Input (1,1) → 0\n\n**No single line can separate these classes!**\n\n### The Solution: Add Hidden Layers!\n\nA **Multi-Layer Perceptron (MLP)** has:\n1. **Input layer**: our features\n2. **Hidden layer(s)**: intermediate transformations\n3. **Output layer**: final predictions\n\n## Architecture of 1-Hidden-Layer MLP\n\n```\nInput (X)  →  Hidden Layer  →  Output Layer  →  Prediction (ŷ)\n  (n_features)    (n_hidden)      (n_outputs)\n```\n\n### Forward Pass:\n\n1. **Hidden layer**: \n   - $z_1 = XW_1 + b_1$\n   - $a_1 = \\sigma(z_1)$ (apply activation)\n\n2. **Output layer**:\n   - $z_2 = a_1W_2 + b_2$\n   - $\\hat{y} = \\sigma(z_2)$ (for binary classification)\n\nWhere:\n- $W_1, b_1$ = weights and bias for hidden layer\n- $W_2, b_2$ = weights and bias for output layer\n- $\\sigma$ = activation function (sigmoid, ReLU, etc.)\n\n### Why Hidden Layers Work:\n\n- **First layer**: Learns useful feature combinations\n- **Second layer**: Combines these features to make final decision\n- Together: Can learn **non-linear** decision boundaries!\n\nThink of it like:\n- Hidden layer = creating new, more useful features\n- Output layer = using these features for prediction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Activation Functions\n\n### Why Activation Functions?\n\nWithout activation functions:\n- $z_2 = (XW_1 + b_1)W_2 + b_2 = X(W_1W_2) + (b_1W_2 + b_2)$\n- This is just another linear transformation!\n- Multiple layers without activation = single layer\n\n**Activation functions add non-linearity!**\n\n### Common Activation Functions:\n\n1. **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n   - Range: $(0, 1)$\n   - Derivative: $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n   - Problem: Vanishing gradients for large $|z|$\n\n2. **ReLU (Rectified Linear Unit)**: $f(z) = \\max(0, z)$\n   - Range: $[0, \\infty)$\n   - Derivative: $f'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n   - Advantages: Fast, no vanishing gradient for $z > 0$\n   - Most popular for hidden layers!\n\n3. **Tanh**: $f(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n   - Range: $(-1, 1)$\n   - Derivative: $f'(z) = 1 - f(z)^2$\n   - Like sigmoid but centered at 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions and their derivatives\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "# Visualize activation functions\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(z, sigmoid(z), linewidth=3, color='#2E86AB')\n",
    "axes[0, 0].set_title('Sigmoid: σ(z) = 1/(1 + e⁻ᶻ)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Output', fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0, 0].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[1, 0].plot(z, sigmoid_derivative(z), linewidth=3, color='#A23B72')\n",
    "axes[1, 0].set_title('Sigmoid Derivative', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('z', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Derivative', fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1, 0].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[0, 1].plot(z, relu(z), linewidth=3, color='#2E86AB')\n",
    "axes[0, 1].set_title('ReLU: f(z) = max(0, z)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0, 1].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[1, 1].plot(z, relu_derivative(z), linewidth=3, color='#A23B72')\n",
    "axes[1, 1].set_title('ReLU Derivative', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('z', fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1, 1].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "# Tanh\n",
    "axes[0, 2].plot(z, tanh(z), linewidth=3, color='#2E86AB')\n",
    "axes[0, 2].set_title('Tanh: f(z) = (eᶻ - e⁻ᶻ)/(eᶻ + e⁻ᶻ)', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[0, 2].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "axes[1, 2].plot(z, tanh_derivative(z), linewidth=3, color='#A23B72')\n",
    "axes[1, 2].set_title('Tanh Derivative', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('z', fontsize=11)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1, 2].axvline(x=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Non-Linear Data\n",
    "\n",
    "Let's create data that **cannot** be separated by a straight line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two types of non-linear datasets\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.1, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# We'll use moons dataset\n",
    "X, y = X_moons, y_moons\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Moons dataset\n",
    "axes[0].scatter(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1],\n",
    "                label='Class 0', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0].scatter(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1],\n",
    "                label='Class 1', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Moons Dataset (Non-Linear)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Circles dataset\n",
    "axes[1].scatter(X_circles[y_circles == 0, 0], X_circles[y_circles == 0, 1],\n",
    "                label='Class 0', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[1].scatter(X_circles[y_circles == 1, 0], X_circles[y_circles == 1, 1],\n",
    "                label='Class 1', alpha=0.7, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Circles Dataset (Non-Linear)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Understanding Backpropagation\n\n**Backpropagation** = computing gradients for all layers using the **chain rule**.\n\n### Forward Pass (recap):\n\n1. Hidden layer: $z_1 = XW_1 + b_1$, $a_1 = \\sigma(z_1)$\n2. Output layer: $z_2 = a_1W_2 + b_2$, $\\hat{y} = \\sigma(z_2)$\n3. Loss: $L = -\\frac{1}{n} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n\n### Backward Pass (computing gradients):\n\nWe need gradients for: $W_1, b_1, W_2, b_2$\n\n**Start from the output and work backwards!**\n\n#### Layer 2 (Output Layer) Gradients:\n\n**Step 1:** Gradient of loss w.r.t. output\n$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\left(\\frac{y}{\\hat{y}} - \\frac{1-y}{1-\\hat{y}}\\right)$$\n\n**Step 2:** Gradient w.r.t. $z_2$ (before activation)\n$$\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\times \\sigma'(z_2)$$\n\nFor binary cross-entropy + sigmoid: $$\\boxed{\\frac{\\partial L}{\\partial z_2} = \\hat{y} - y}$$\n\n**Step 3:** Gradients for $W_2$ and $b_2$\n$$\\boxed{\\frac{\\partial L}{\\partial W_2} = \\frac{1}{n} a_1^T(\\hat{y} - y)}$$\n$$\\boxed{\\frac{\\partial L}{\\partial b_2} = \\frac{1}{n} \\sum (\\hat{y} - y)}$$\n\n#### Layer 1 (Hidden Layer) Gradients:\n\nThis is where backpropagation really shines!\n\n**Step 4:** Gradient w.r.t. $a_1$ (hidden layer activations)\n$$\\frac{\\partial L}{\\partial a_1} = \\frac{\\partial L}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial a_1}$$\n\nSince $z_2 = a_1W_2 + b_2$, so $\\frac{\\partial z_2}{\\partial a_1} = W_2$\n\n$$\\boxed{\\frac{\\partial L}{\\partial a_1} = (\\hat{y} - y)W_2^T}$$\n\n**Step 5:** Gradient w.r.t. $z_1$ (before activation)\n$$\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\frac{\\partial a_1}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\times \\sigma'(z_1)$$\n\n$$\\boxed{\\frac{\\partial L}{\\partial z_1} = [(\\hat{y} - y)W_2^T] \\odot \\sigma'(z_1)}$$\n\n($\\odot$ means element-wise multiplication)\n\n**Step 6:** Gradients for $W_1$ and $b_1$\n$$\\boxed{\\frac{\\partial L}{\\partial W_1} = \\frac{1}{n} X^T\\left(\\frac{\\partial L}{\\partial z_1}\\right)}$$\n$$\\boxed{\\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} \\sum \\left(\\frac{\\partial L}{\\partial z_1}\\right)}$$\n\n### Key Insights:\n\n1. **Chain Rule**: Errors propagate backwards through layers\n2. **Reuse Computations**: We store $z_1, a_1, z_2$ from forward pass\n3. **Matrix Operations**: All gradients computed efficiently with matrices\n4. **Same Pattern**: Each layer follows same gradient pattern\n\n### Backpropagation in Plain English:\n\n1. **Output layer error**: How wrong were we? $(\\hat{y} - y)$\n2. **Output layer updates**: Adjust $W_2$ and $b_2$ to fix this error\n3. **Propagate error back**: How much did hidden layer contribute? (error $\\times W_2^T$)\n4. **Hidden layer error**: Scale by activation derivative (only update active neurons)\n5. **Hidden layer updates**: Adjust $W_1$ and $b_1$ to fix propagated error"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement MLP with 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_1Hidden:\n",
    "    \"\"\"Multi-Layer Perceptron with 1 Hidden Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_output, learning_rate=0.1, n_iterations=1000, activation='relu'):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize weights (He initialization for ReLU, Xavier for others)\n",
    "        if activation == 'relu':\n",
    "            self.W1 = np.random.randn(n_input, n_hidden) * np.sqrt(2.0 / n_input)\n",
    "            self.W2 = np.random.randn(n_hidden, n_output) * np.sqrt(2.0 / n_hidden)\n",
    "        else:\n",
    "            self.W1 = np.random.randn(n_input, n_hidden) * np.sqrt(1.0 / n_input)\n",
    "            self.W2 = np.random.randn(n_hidden, n_output) * np.sqrt(1.0 / n_hidden)\n",
    "        \n",
    "        self.b1 = np.zeros((1, n_hidden))\n",
    "        self.b2 = np.zeros((1, n_output))\n",
    "        \n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Select activation functions\n",
    "        if activation == 'relu':\n",
    "            self.act_fn = relu\n",
    "            self.act_derivative = relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.act_fn = tanh\n",
    "            self.act_derivative = tanh_derivative\n",
    "        else:  # sigmoid\n",
    "            self.act_fn = sigmoid\n",
    "            self.act_derivative = sigmoid_derivative\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass - compute predictions\"\"\"\n",
    "        # Layer 1: Input → Hidden\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.act_fn(self.z1)\n",
    "        \n",
    "        # Layer 2: Hidden → Output\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = sigmoid(self.z2)  # Sigmoid for binary classification\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_cost(self, y, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        n = len(y)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        cost = -(1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass - compute gradients\"\"\"\n",
    "        n = len(y)\n",
    "        y = y.reshape(-1, 1)  # Ensure correct shape\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y  # Shape: (n, 1)\n",
    "        dW2 = (1/n) * (self.a1.T @ dz2)  # Shape: (n_hidden, n_output)\n",
    "        db2 = (1/n) * np.sum(dz2, axis=0, keepdims=True)  # Shape: (1, n_output)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        da1 = dz2 @ self.W2.T  # Shape: (n, n_hidden)\n",
    "        dz1 = da1 * self.act_derivative(self.z1)  # Element-wise multiplication\n",
    "        dW1 = (1/n) * (X.T @ dz1)  # Shape: (n_input, n_hidden)\n",
    "        db1 = (1/n) * np.sum(dz1, axis=0, keepdims=True)  # Shape: (1, n_hidden)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        for i in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.W1 -= self.lr * dW1\n",
    "            self.b1 -= self.lr * db1\n",
    "            self.W2 -= self.lr * dW2\n",
    "            self.b2 -= self.lr * db2\n",
    "            \n",
    "            if verbose and (i % 100 == 0 or i == self.n_iterations - 1):\n",
    "                print(f\"Iteration {i:4d} | Cost: {cost:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "n_input = X_train.shape[1]  # 2 features\n",
    "n_hidden = 10  # 10 neurons in hidden layer\n",
    "n_output = 1  # Binary classification\n",
    "\n",
    "model = MLP_1Hidden(\n",
    "    n_input=n_input,\n",
    "    n_hidden=n_hidden,\n",
    "    n_output=n_output,\n",
    "    learning_rate=0.5,\n",
    "    n_iterations=2000,\n",
    "    activation='relu'\n",
    ")\n",
    "\n",
    "print(f\"Network Architecture:\")\n",
    "print(f\"Input Layer:  {n_input} neurons\")\n",
    "print(f\"Hidden Layer: {n_hidden} neurons (ReLU activation)\")\n",
    "print(f\"Output Layer: {n_output} neuron (Sigmoid activation)\")\n",
    "print(f\"\\nTotal Parameters: {model.W1.size + model.b1.size + model.W2.size + model.b2.size}\")\n",
    "print(f\"  W1: {model.W1.shape}, b1: {model.b1.shape}\")\n",
    "print(f\"  W2: {model.W2.shape}, b2: {model.b2.shape}\")\n",
    "print()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_train_pred == y_train)\n",
    "test_accuracy = np.mean(y_test_pred == y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Cost history\n",
    "axes[0, 0].plot(model.cost_history, linewidth=2, color='#2E86AB')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Cost (Binary Cross-Entropy)', fontsize=12)\n",
    "axes[0, 0].set_title('Learning Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "contour = axes[0, 1].contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[0, 1].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[0, 1].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1],\n",
    "                   label='Class 0', alpha=0.8, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1],\n",
    "                   label='Class 1', alpha=0.8, s=50, edgecolors='black', linewidth=0.5)\n",
    "axes[0, 1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0, 1].set_title('Decision Boundary (Training Data)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "plt.colorbar(contour, ax=axes[0, 1], label='P(y=1)')\n",
    "\n",
    "# Plot 3: Confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Pred 0', 'Pred 1'],\n",
    "            yticklabels=['True 0', 'True 1'])\n",
    "axes[1, 0].set_title('Confusion Matrix (Test)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('True Label', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Plot 4: Hidden layer activations\n",
    "# Visualize what hidden neurons learned\n",
    "hidden_activations = model.a1  # From last forward pass (training data)\n",
    "axes[1, 1].imshow(hidden_activations[:50].T, aspect='auto', cmap='viridis', interpolation='nearest')\n",
    "axes[1, 1].set_xlabel('Sample Index (first 50)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Hidden Neuron', fontsize=12)\n",
    "axes[1, 1].set_title('Hidden Layer Activations', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(axes[1, 1].images[0], ax=axes[1, 1], label='Activation Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Different Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different activations\n",
    "activations = ['sigmoid', 'tanh', 'relu']\n",
    "models = {}\n",
    "\n",
    "for act in activations:\n",
    "    print(f\"\\nTraining with {act} activation...\")\n",
    "    model_act = MLP_1Hidden(\n",
    "        n_input=n_input,\n",
    "        n_hidden=n_hidden,\n",
    "        n_output=n_output,\n",
    "        learning_rate=0.5,\n",
    "        n_iterations=2000,\n",
    "        activation=act\n",
    "    )\n",
    "    model_act.fit(X_train, y_train, verbose=False)\n",
    "    models[act] = model_act\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = np.mean(model_act.predict(X_train) == y_train)\n",
    "    test_acc = np.mean(model_act.predict(X_test) == y_test)\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for act, model in models.items():\n",
    "    plt.plot(model.cost_history, label=act, linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Cost', fontsize=12)\n",
    "plt.title('Learning Curves: Different Activations', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare final accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "train_accs = [np.mean(models[act].predict(X_train) == y_train) for act in activations]\n",
    "test_accs = [np.mean(models[act].predict(X_test) == y_test) for act in activations]\n",
    "\n",
    "x_pos = np.arange(len(activations))\n",
    "width = 0.35\n",
    "plt.bar(x_pos - width/2, train_accs, width, label='Train', alpha=0.8)\n",
    "plt.bar(x_pos + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('Activation Function', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x_pos, activations)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### What We Learned:\n\n1. **Neural Networks = Stacked Transformations**\n   - Input → Hidden Layer → Output\n   - Each layer: linear transformation + non-linear activation\n   - Can learn non-linear decision boundaries!\n\n2. **Activation Functions are Crucial**\n   - Without them: multiple layers = single layer (still linear)\n   - **ReLU**: $f(z) = \\max(0, z)$ - most popular for hidden layers\n   - **Sigmoid**: $\\sigma(z) = \\frac{1}{1+e^{-z}}$ - good for output layer (probabilities)\n   - **Tanh**: similar to sigmoid but centered at 0\n\n3. **Forward Pass**\n   - Layer 1: $z_1 = XW_1 + b_1$, $a_1 = \\text{activation}(z_1)$\n   - Layer 2: $z_2 = a_1W_2 + b_2$, $\\hat{y} = \\sigma(z_2)$\n   - Store $z_1, a_1, z_2$ for backward pass!\n\n4. **Backpropagation = Chain Rule**\n   \n   **Output Layer:**\n   - $\\frac{\\partial L}{\\partial z_2} = \\hat{y} - y$ (error)\n   - $\\frac{\\partial L}{\\partial W_2} = \\frac{1}{n} a_1^T(\\hat{y} - y)$\n   - $\\frac{\\partial L}{\\partial b_2} = \\frac{1}{n} \\sum (\\hat{y} - y)$\n   \n   **Hidden Layer:**\n   - $\\frac{\\partial L}{\\partial a_1} = (\\hat{y} - y)W_2^T$ (propagate error back)\n   - $\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\odot \\text{activation}'(z_1)$ (scale by derivative)\n   - $\\frac{\\partial L}{\\partial W_1} = \\frac{1}{n} X^T\\left(\\frac{\\partial L}{\\partial z_1}\\right)$\n   - $\\frac{\\partial L}{\\partial b_1} = \\frac{1}{n} \\sum \\left(\\frac{\\partial L}{\\partial z_1}\\right)$\n\n5. **The Flow of Backpropagation:**\n   ```\n   Forward:  X → z₁ → a₁ → z₂ → ŷ → Loss\n   Backward: X ← ∂W₁ ← ∂z₁ ← ∂a₁ ← ∂z₂ ← (ŷ-y)\n   ```\n\n6. **Weight Initialization Matters**\n   - All zeros → all neurons learn same thing\n   - Too large → exploding gradients\n   - Too small → vanishing gradients\n   - **He initialization** (for ReLU): $W \\sim \\mathcal{N}(0, \\sqrt{2/n_{input}})$\n   - **Xavier initialization** (for sigmoid/tanh): $W \\sim \\mathcal{N}(0, \\sqrt{1/n_{input}})$\n\n7. **Hidden Layers Learn Features**\n   - Each hidden neuron learns to detect a pattern\n   - Output layer combines these patterns\n   - More neurons → more complex patterns\n\n### Mathematical Beauty:\n\nThe gradient formulas might look complex, but they follow a beautiful pattern:\n\n**For any layer:**\n1. Get error from next layer\n2. Multiply by activation derivative (element-wise)\n3. Compute weight gradient: input$^T$ @ error\n4. Compute bias gradient: sum of errors\n5. Pass error to previous layer: error @ weights$^T$\n\nThis pattern works for any number of layers!\n\n### Next Steps:\n\n- Add more hidden layers (deep learning!)\n- Different architectures\n- Regularization techniques\n- More complex activations and losses"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}