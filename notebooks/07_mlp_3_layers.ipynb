{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron with 3 Hidden Layers\n",
    "\n",
    "In this notebook, we'll implement a deeper neural network with **3 hidden layers**. This demonstrates how the backpropagation algorithm scales to deeper architectures.\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "Our network will have:\n",
    "- **Input layer**: $n$ features\n",
    "- **Hidden layer 1**: $h_1$ neurons with activation function $\\sigma_1$\n",
    "- **Hidden layer 2**: $h_2$ neurons with activation function $\\sigma_2$\n",
    "- **Hidden layer 3**: $h_3$ neurons with activation function $\\sigma_3$\n",
    "- **Output layer**: 1 neuron (binary classification with sigmoid)\n",
    "\n",
    "## Why Deeper Networks?\n",
    "\n",
    "Deeper networks can learn more complex hierarchical features:\n",
    "- **Layer 1**: Simple patterns (edges, basic shapes)\n",
    "- **Layer 2**: Combinations of simple patterns\n",
    "- **Layer 3**: High-level features\n",
    "- **Output**: Final decision\n",
    "\n",
    "This is the foundation of deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward Propagation\n",
    "\n",
    "Let's denote:\n",
    "- $X$: input matrix (shape: $m \\times n$)\n",
    "- $W^{(1)}$, $b^{(1)}$: weights and bias for layer 1\n",
    "- $W^{(2)}$, $b^{(2)}$: weights and bias for layer 2\n",
    "- $W^{(3)}$, $b^{(3)}$: weights and bias for layer 3\n",
    "- $W^{(4)}$, $b^{(4)}$: weights and bias for output layer\n",
    "\n",
    "### Forward Pass Equations:\n",
    "\n",
    "**Layer 1:**\n",
    "$$z^{(1)} = XW^{(1)} + b^{(1)}$$\n",
    "$$a^{(1)} = \\sigma_1(z^{(1)})$$\n",
    "\n",
    "**Layer 2:**\n",
    "$$z^{(2)} = a^{(1)}W^{(2)} + b^{(2)}$$\n",
    "$$a^{(2)} = \\sigma_2(z^{(2)})$$\n",
    "\n",
    "**Layer 3:**\n",
    "$$z^{(3)} = a^{(2)}W^{(3)} + b^{(3)}$$\n",
    "$$a^{(3)} = \\sigma_3(z^{(3)})$$\n",
    "\n",
    "**Output Layer:**\n",
    "$$z^{(4)} = a^{(3)}W^{(4)} + b^{(4)}$$\n",
    "$$\\hat{y} = \\text{sigmoid}(z^{(4)})$$\n",
    "\n",
    "### Loss Function (Binary Cross-Entropy):\n",
    "\n",
    "$$L = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation for 3 Hidden Layers\n",
    "\n",
    "Working backwards from the output:\n",
    "\n",
    "### Output Layer Gradients:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial z^{(4)}} = \\hat{y} - y}$$\n",
    "\n",
    "Let $\\delta^{(4)} = \\hat{y} - y$ (shape: $m \\times 1$)\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial W^{(4)}} = \\frac{1}{m}(a^{(3)})^T \\delta^{(4)}}$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial b^{(4)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta^{(4)}_i}$$\n",
    "\n",
    "### Hidden Layer 3 Gradients:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{(3)}} = \\delta^{(4)}(W^{(4)})^T$$\n",
    "\n",
    "$$\\delta^{(3)} = \\frac{\\partial L}{\\partial a^{(3)}} \\odot \\sigma_3'(z^{(3)})$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial W^{(3)}} = \\frac{1}{m}(a^{(2)})^T \\delta^{(3)}}$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial b^{(3)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta^{(3)}_i}$$\n",
    "\n",
    "### Hidden Layer 2 Gradients:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{(2)}} = \\delta^{(3)}(W^{(3)})^T$$\n",
    "\n",
    "$$\\delta^{(2)} = \\frac{\\partial L}{\\partial a^{(2)}} \\odot \\sigma_2'(z^{(2)})$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial W^{(2)}} = \\frac{1}{m}(a^{(1)})^T \\delta^{(2)}}$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial b^{(2)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta^{(2)}_i}$$\n",
    "\n",
    "### Hidden Layer 1 Gradients:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{(1)}} = \\delta^{(2)}(W^{(2)})^T$$\n",
    "\n",
    "$$\\delta^{(1)} = \\frac{\\partial L}{\\partial a^{(1)}} \\odot \\sigma_1'(z^{(1)})$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{1}{m}X^T \\delta^{(1)}}$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial b^{(1)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta^{(1)}_i}$$\n",
    "\n",
    "### Pattern Recognition:\n",
    "\n",
    "Notice the **universal pattern** for any layer $l$:\n",
    "1. Compute error signal: $\\delta^{(l)} = \\frac{\\partial L}{\\partial a^{(l)}} \\odot \\sigma'(z^{(l)})$\n",
    "2. Weight gradient: $\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{1}{m}(a^{(l-1)})^T \\delta^{(l)}$\n",
    "3. Bias gradient: $\\frac{\\partial L}{\\partial b^{(l)}} = \\frac{1}{m}\\sum \\delta^{(l)}$\n",
    "4. Propagate back: $\\frac{\\partial L}{\\partial a^{(l-1)}} = \\delta^{(l)}(W^{(l)})^T$\n",
    "\n",
    "This pattern allows us to build networks of **any depth**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and their derivatives\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_3Hidden:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with 3 hidden layers.\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Hidden1 -> Hidden2 -> Hidden3 -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, hidden3_size, \n",
    "                 output_size=1, activation='relu', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features\n",
    "        hidden1_size : int\n",
    "            Number of neurons in first hidden layer\n",
    "        hidden2_size : int\n",
    "            Number of neurons in second hidden layer\n",
    "        hidden3_size : int\n",
    "            Number of neurons in third hidden layer\n",
    "        output_size : int\n",
    "            Number of output neurons (default: 1 for binary classification)\n",
    "        activation : str\n",
    "            Activation function: 'relu', 'sigmoid', or 'tanh'\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.act_derivative = relu_derivative\n",
    "            init_scale = np.sqrt(2.0)  # He initialization\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.act_derivative = sigmoid_derivative\n",
    "            init_scale = 1.0  # Xavier initialization\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.act_derivative = tanh_derivative\n",
    "            init_scale = 1.0  # Xavier initialization\n",
    "        else:\n",
    "            raise ValueError(\"Activation must be 'relu', 'sigmoid', or 'tanh'\")\n",
    "        \n",
    "        # Initialize weights with appropriate scaling\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * init_scale / np.sqrt(input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * init_scale / np.sqrt(hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        \n",
    "        self.W3 = np.random.randn(hidden2_size, hidden3_size) * init_scale / np.sqrt(hidden2_size)\n",
    "        self.b3 = np.zeros((1, hidden3_size))\n",
    "        \n",
    "        self.W4 = np.random.randn(hidden3_size, output_size) * init_scale / np.sqrt(hidden3_size)\n",
    "        self.b4 = np.zeros((1, output_size))\n",
    "        \n",
    "        # For storing intermediate values during forward pass\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "        self.z3 = None\n",
    "        self.a3 = None\n",
    "        self.z4 = None\n",
    "        self.a4 = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (m, n)\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : ndarray, shape (m, 1)\n",
    "            Network predictions\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.activation(self.z2)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.z3 = self.a2 @ self.W3 + self.b3\n",
    "        self.a3 = self.activation(self.z3)\n",
    "        \n",
    "        # Output layer (sigmoid for binary classification)\n",
    "        self.z4 = self.a3 @ self.W4 + self.b4\n",
    "        self.a4 = sigmoid(self.z4)\n",
    "        \n",
    "        return self.a4\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (m, n)\n",
    "            Input data\n",
    "        y : ndarray, shape (m, 1)\n",
    "            True labels\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients (δ⁴ = ŷ - y)\n",
    "        delta4 = self.a4 - y\n",
    "        dW4 = (1/m) * (self.a3.T @ delta4)\n",
    "        db4 = (1/m) * np.sum(delta4, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 3 gradients\n",
    "        delta3 = (delta4 @ self.W4.T) * self.act_derivative(self.z3)\n",
    "        dW3 = (1/m) * (self.a2.T @ delta3)\n",
    "        db3 = (1/m) * np.sum(delta3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        delta2 = (delta3 @ self.W3.T) * self.act_derivative(self.z2)\n",
    "        dW2 = (1/m) * (self.a1.T @ delta2)\n",
    "        db2 = (1/m) * np.sum(delta2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        delta1 = (delta2 @ self.W2.T) * self.act_derivative(self.z1)\n",
    "        dW1 = (1/m) * (X.T @ delta1)\n",
    "        db1 = (1/m) * np.sum(delta1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases using gradient descent\n",
    "        self.W4 -= self.lr * dW4\n",
    "        self.b4 -= self.lr * db4\n",
    "        self.W3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : ndarray\n",
    "            True labels\n",
    "        y_pred : ndarray\n",
    "            Predicted probabilities\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (m, n)\n",
    "            Training data\n",
    "        y : ndarray, shape (m, 1)\n",
    "            Training labels\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        losses : list\n",
    "            Loss history\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, predictions)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Input data\n",
    "        threshold : float\n",
    "            Decision threshold\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : ndarray\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training on Complex Dataset\n",
    "\n",
    "Let's test our 3-layer network on a complex dataset that requires deep features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex dataset (nested circles)\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train.ravel() == 0, 0], X_train[y_train.ravel() == 0, 1], \n",
    "           c='blue', label='Class 0', alpha=0.6, edgecolors='k')\n",
    "plt.scatter(X_train[y_train.ravel() == 1, 0], X_train[y_train.ravel() == 1, 1], \n",
    "           c='red', label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Complex Dataset: Nested Circles')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the 3-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the 3-layer MLP\n",
    "mlp3 = MLP_3Hidden(\n",
    "    input_size=2,\n",
    "    hidden1_size=16,\n",
    "    hidden2_size=12,\n",
    "    hidden3_size=8,\n",
    "    output_size=1,\n",
    "    activation='relu',\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "print(\"Training 3-Layer MLP...\\n\")\n",
    "losses = mlp3.train(X_train_scaled, y_train, epochs=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Binary Cross-Entropy)')\n",
    "plt.title('Learning Curve: 3-Layer MLP')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = mlp3.predict(X_train_scaled)\n",
    "y_test_pred = mlp3.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_train_pred == y_train) * 100\n",
    "test_accuracy = np.mean(y_test_pred == y_test) * 100\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, scaler, title):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for binary classification.\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Scale mesh points\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_scaled = scaler.transform(mesh_points)\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.forward(mesh_scaled)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    plt.colorbar(label='Predicted Probability')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y.ravel() == 0, 0], X[y.ravel() == 0, 1], \n",
    "               c='blue', label='Class 0', edgecolors='k', s=50)\n",
    "    plt.scatter(X[y.ravel() == 1, 0], X[y.ravel() == 1, 1], \n",
    "               c='red', label='Class 1', edgecolors='k', s=50)\n",
    "    \n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(mlp3, X_test, y_test, scaler, \n",
    "                      '3-Layer MLP Decision Boundary (Test Set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Different Depths\n",
    "\n",
    "Let's compare networks with different depths to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train networks with different depths\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate more complex dataset\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "y_moons = y_moons.reshape(-1, 1)\n",
    "\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_m = StandardScaler()\n",
    "X_train_m_scaled = scaler_m.fit_transform(X_train_m)\n",
    "X_test_m_scaled = scaler_m.transform(X_test_m)\n",
    "\n",
    "# 1-Layer MLP\n",
    "from sklearn.neural_network import MLPClassifier as MLP_sklearn\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training 1-Layer MLP...\")\n",
    "mlp1 = MLP_3Hidden(2, 16, 8, 4, 1, 'relu', 0.1)\n",
    "# Simulate 1-layer by setting middle layers small\n",
    "losses1 = mlp1.train(X_train_m_scaled, y_train_m, epochs=1000, verbose=False)\n",
    "acc1 = np.mean(mlp1.predict(X_test_m_scaled) == y_test_m) * 100\n",
    "results['1-Layer'] = acc1\n",
    "print(f\"1-Layer Test Accuracy: {acc1:.2f}%\\n\")\n",
    "\n",
    "print(\"Training 2-Layer MLP...\")\n",
    "mlp2 = MLP_3Hidden(2, 16, 12, 6, 1, 'relu', 0.1)\n",
    "losses2 = mlp2.train(X_train_m_scaled, y_train_m, epochs=1000, verbose=False)\n",
    "acc2 = np.mean(mlp2.predict(X_test_m_scaled) == y_test_m) * 100\n",
    "results['2-Layer'] = acc2\n",
    "print(f\"2-Layer Test Accuracy: {acc2:.2f}%\\n\")\n",
    "\n",
    "print(\"Training 3-Layer MLP...\")\n",
    "mlp3_moons = MLP_3Hidden(2, 20, 16, 12, 1, 'relu', 0.1)\n",
    "losses3 = mlp3_moons.train(X_train_m_scaled, y_train_m, epochs=1000, verbose=False)\n",
    "acc3 = np.mean(mlp3_moons.predict(X_test_m_scaled) == y_test_m) * 100\n",
    "results['3-Layer'] = acc3\n",
    "print(f\"3-Layer Test Accuracy: {acc3:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = [mlp1, mlp2, mlp3_moons]\n",
    "titles = ['1-Layer MLP', '2-Layer MLP', '3-Layer MLP']\n",
    "accs = [acc1, acc2, acc3]\n",
    "\n",
    "for idx, (model, title, acc) in enumerate(zip(models, titles, accs)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_test_m[:, 0].min() - 0.5, X_test_m[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_test_m[:, 1].min() - 0.5, X_test_m[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_scaled = scaler_m.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = model.forward(mesh_scaled).reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    ax.scatter(X_test_m[y_test_m.ravel() == 0, 0], X_test_m[y_test_m.ravel() == 0, 1],\n",
    "              c='blue', edgecolors='k', s=30, label='Class 0')\n",
    "    ax.scatter(X_test_m[y_test_m.ravel() == 1, 0], X_test_m[y_test_m.ravel() == 1, 1],\n",
    "              c='red', edgecolors='k', s=30, label='Class 1')\n",
    "    ax.set_title(f'{title}\\nAccuracy: {acc:.2f}%')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learning curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses1, label='1-Layer MLP', linewidth=2, alpha=0.8)\n",
    "plt.plot(losses2, label='2-Layer MLP', linewidth=2, alpha=0.8)\n",
    "plt.plot(losses3, label='3-Layer MLP', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### Backpropagation Pattern\n",
    "\n",
    "The beauty of backpropagation is its **universal pattern**:\n",
    "\n",
    "For any layer $l$ in the network:\n",
    "\n",
    "1. **Error signal**: $\\delta^{(l)} = \\frac{\\partial L}{\\partial a^{(l)}} \\odot \\sigma'(z^{(l)})$\n",
    "2. **Weight gradient**: $\\frac{\\partial L}{\\partial W^{(l)}} = (a^{(l-1)})^T \\delta^{(l)}$\n",
    "3. **Bias gradient**: $\\frac{\\partial L}{\\partial b^{(l)}} = \\sum \\delta^{(l)}$\n",
    "4. **Propagate**: $\\frac{\\partial L}{\\partial a^{(l-1)}} = \\delta^{(l)}(W^{(l)})^T$\n",
    "\n",
    "This pattern works for networks of **any depth**!\n",
    "\n",
    "### Depth vs Performance\n",
    "\n",
    "- **Deeper networks** can learn more complex patterns\n",
    "- But they also:\n",
    "  - Take longer to train\n",
    "  - May overfit if not enough data\n",
    "  - Can suffer from vanishing gradients\n",
    "  \n",
    "Choose depth based on:\n",
    "- Problem complexity\n",
    "- Available data\n",
    "- Computational resources\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand deep networks, you're ready for:\n",
    "- **Regularization**: Preventing overfitting (dropout, L2)\n",
    "- **Batch normalization**: Stabilizing training\n",
    "- **Advanced optimizers**: Adam, RMSprop\n",
    "- **Convolutional networks**: For images\n",
    "- **Recurrent networks**: For sequences\n",
    "\n",
    "You now have the foundation of **deep learning**! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
