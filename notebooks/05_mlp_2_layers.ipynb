{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Layer Perceptron (2 Hidden Layers) - Deep Learning!\n\n## From Shallow to Deep\n\nIn the previous notebook, we had:\n- Input → **1 Hidden Layer** → Output\n\nNow we'll build:\n- Input → **Hidden Layer 1** → **Hidden Layer 2** → Output\n\nThis is **deep learning** - multiple layers of transformations!\n\n### Why Go Deeper?\n\n1. **Hierarchical Feature Learning**\n   - Layer 1: Learn simple features (edges, basic patterns)\n   - Layer 2: Combine simple features into complex ones (shapes, parts)\n   - Output: Combine complex features for final decision\n\n2. **More Expressive**\n   - Can approximate more complex functions\n   - Better at modeling intricate patterns\n\n3. **Parameter Efficiency**\n   - Sometimes 2 smaller layers learn better than 1 huge layer\n   - Fewer parameters but more representation power\n\n## Architecture\n\n```\nInput (X)  →  Layer 1  →  Layer 2  →  Output  →  ŷ\n (n_input)    (n_h1)      (n_h2)     (n_out)\n```\n\n### Forward Pass:\n\n1. **Layer 1**: $z_1 = XW_1 + b_1$,     $a_1 = \\text{activation}(z_1)$\n2. **Layer 2**: $z_2 = a_1W_2 + b_2$,   $a_2 = \\text{activation}(z_2)$\n3. **Output**:  $z_3 = a_2W_3 + b_3$,   $\\hat{y} = \\sigma(z_3)$\n\n### Backward Pass:\n\nThe beautiful part: **same pattern repeats for each layer!**\n\n**Output Layer (Layer 3):**\n- $\\frac{\\partial L}{\\partial z_3} = \\hat{y} - y$\n- $\\frac{\\partial L}{\\partial W_3} = a_2^T\\left(\\frac{\\partial L}{\\partial z_3}\\right)$\n- $\\frac{\\partial L}{\\partial b_3} = \\sum\\left(\\frac{\\partial L}{\\partial z_3}\\right)$\n\n**Hidden Layer 2:**\n- $\\frac{\\partial L}{\\partial a_2} = \\left(\\frac{\\partial L}{\\partial z_3}\\right)W_3^T$\n- $\\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial a_2} \\odot \\text{activation}'(z_2)$\n- $\\frac{\\partial L}{\\partial W_2} = a_1^T\\left(\\frac{\\partial L}{\\partial z_2}\\right)$\n- $\\frac{\\partial L}{\\partial b_2} = \\sum\\left(\\frac{\\partial L}{\\partial z_2}\\right)$\n\n**Hidden Layer 1:**\n- $\\frac{\\partial L}{\\partial a_1} = \\left(\\frac{\\partial L}{\\partial z_2}\\right)W_2^T$\n- $\\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial a_1} \\odot \\text{activation}'(z_1)$\n- $\\frac{\\partial L}{\\partial W_1} = X^T\\left(\\frac{\\partial L}{\\partial z_1}\\right)$\n- $\\frac{\\partial L}{\\partial b_1} = \\sum\\left(\\frac{\\partial L}{\\partial z_1}\\right)$\n\nSee the pattern? **Each layer follows the same backward recipe!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions and derivatives\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Complex Dataset\n",
    "\n",
    "Let's create a more challenging dataset that benefits from deeper networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple datasets\n",
    "X_moons, y_moons = make_moons(n_samples=800, noise=0.15, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=800, noise=0.1, factor=0.4, random_state=42)\n",
    "\n",
    "# We'll use circles (harder problem)\n",
    "X, y = X_circles, y_circles\n",
    "\n",
    "# Split and normalize\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Moons\n",
    "axes[0].scatter(X_moons[y_moons == 0, 0], X_moons[y_moons == 0, 1],\n",
    "                label='Class 0', alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "axes[0].scatter(X_moons[y_moons == 1, 0], X_moons[y_moons == 1, 1],\n",
    "                label='Class 1', alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_title('Moons Dataset', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Circles (our choice)\n",
    "axes[1].scatter(X_circles[y_circles == 0, 0], X_circles[y_circles == 0, 1],\n",
    "                label='Class 0', alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "axes[1].scatter(X_circles[y_circles == 1, 0], X_circles[y_circles == 1, 1],\n",
    "                label='Class 1', alpha=0.7, s=40, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_title('Circles Dataset (More Challenging!)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implement Deep MLP\n",
    "\n",
    "Let's build a flexible MLP that can have any number of layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Deep:\n",
    "    \"\"\"Multi-Layer Perceptron with configurable hidden layers\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, learning_rate=0.1, n_iterations=1000, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes [n_input, n_hidden1, n_hidden2, ..., n_output]\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "            n_iterations: Number of training iterations\n",
    "            activation: Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_layers = len(layer_sizes) - 1  # Number of weight matrices\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Select activation functions\n",
    "        if activation == 'relu':\n",
    "            self.act_fn = relu\n",
    "            self.act_derivative = relu_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.act_fn = tanh\n",
    "            self.act_derivative = tanh_derivative\n",
    "        else:\n",
    "            self.act_fn = sigmoid\n",
    "            self.act_derivative = sigmoid_derivative\n",
    "        \n",
    "        # Initialize weights and biases for all layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # He initialization for ReLU, Xavier for others\n",
    "            if activation == 'relu':\n",
    "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1.0 / layer_sizes[i])\n",
    "            \n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        self.cost_history = []\n",
    "        \n",
    "        # Store activations for backward pass\n",
    "        self.z_cache = []  # Pre-activation values\n",
    "        self.a_cache = []  # Post-activation values\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through all layers\"\"\"\n",
    "        self.z_cache = []\n",
    "        self.a_cache = [X]  # Input is a_0\n",
    "        \n",
    "        a = X\n",
    "        \n",
    "        # Forward through all layers\n",
    "        for i in range(self.n_layers):\n",
    "            z = a @ self.weights[i] + self.biases[i]\n",
    "            self.z_cache.append(z)\n",
    "            \n",
    "            # Use activation function (sigmoid for output layer, chosen activation for hidden)\n",
    "            if i == self.n_layers - 1:  # Output layer\n",
    "                a = sigmoid(z)  # Binary classification\n",
    "            else:  # Hidden layers\n",
    "                a = self.act_fn(z)\n",
    "            \n",
    "            self.a_cache.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def compute_cost(self, y, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        n = len(y)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        cost = -(1/n) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass - compute gradients for all layers\"\"\"\n",
    "        n = len(y)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # Initialize gradient storage\n",
    "        dW = [None] * self.n_layers\n",
    "        db = [None] * self.n_layers\n",
    "        \n",
    "        # Output layer gradient (Layer L)\n",
    "        # For binary cross-entropy + sigmoid, gradient simplifies to (y_pred - y)\n",
    "        dz = self.a_cache[-1] - y\n",
    "        \n",
    "        # Backward through all layers\n",
    "        for i in range(self.n_layers - 1, -1, -1):\n",
    "            # Gradient w.r.t. weights and biases\n",
    "            dW[i] = (1/n) * (self.a_cache[i].T @ dz)\n",
    "            db[i] = (1/n) * np.sum(dz, axis=0, keepdims=True)\n",
    "            \n",
    "            # Propagate gradient to previous layer\n",
    "            if i > 0:  # Not the first layer\n",
    "                da = dz @ self.weights[i].T\n",
    "                dz = da * self.act_derivative(self.z_cache[i-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(y, y_pred)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward(X, y)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for i in range(self.n_layers):\n",
    "                self.weights[i] -= self.lr * dW[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "            \n",
    "            if verbose and (iteration % 200 == 0 or iteration == self.n_iterations - 1):\n",
    "                print(f\"Iteration {iteration:4d} | Cost: {cost:.6f}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Models with Different Depths\n",
    "\n",
    "Let's compare:\n",
    "1. Shallow: [2, 10, 1] (1 hidden layer)\n",
    "2. Deep: [2, 16, 8, 1] (2 hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Shallow (1 hidden layer)\n",
    "print(\"Training Shallow Network (1 hidden layer)...\")\n",
    "print(\"Architecture: Input(2) → Hidden(10) → Output(1)\")\n",
    "print()\n",
    "model_shallow = MLP_Deep(\n",
    "    layer_sizes=[2, 10, 1],\n",
    "    learning_rate=0.5,\n",
    "    n_iterations=2000,\n",
    "    activation='relu'\n",
    ")\n",
    "model_shallow.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Deep (2 hidden layers)\n",
    "print(\"\\nTraining Deep Network (2 hidden layers)...\")\n",
    "print(\"Architecture: Input(2) → Hidden1(16) → Hidden2(8) → Output(1)\")\n",
    "print()\n",
    "model_deep = MLP_Deep(\n",
    "    layer_sizes=[2, 16, 8, 1],\n",
    "    learning_rate=0.5,\n",
    "    n_iterations=2000,\n",
    "    activation='relu'\n",
    ")\n",
    "model_deep.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "models = {\n",
    "    'Shallow (1 layer)': model_shallow,\n",
    "    'Deep (2 layers)': model_deep\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "    test_acc = np.mean(model.predict(X_test) == y_test)\n",
    "    \n",
    "    total_params = sum(w.size + b.size for w, b in zip(model.weights, model.biases))\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total Parameters: {total_params}\")\n",
    "    print(f\"  Train Accuracy:   {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:    {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "axes[0, 0].plot(model_shallow.cost_history, label='Shallow (1 layer)', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(model_deep.cost_history, label='Deep (2 layers)', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Cost', fontsize=12)\n",
    "axes[0, 0].set_title('Learning Curves', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Helper function to plot decision boundary\n",
    "def plot_decision_boundary(ax, model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    ax.scatter(X[y == 0, 0], X[y == 0, 1], label='Class 0', \n",
    "               alpha=0.8, s=30, edgecolors='black', linewidth=0.5)\n",
    "    ax.scatter(X[y == 1, 0], X[y == 1, 1], label='Class 1',\n",
    "               alpha=0.8, s=30, edgecolors='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Feature 1', fontsize=11)\n",
    "    ax.set_ylabel('Feature 2', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    return contour\n",
    "\n",
    "# Plot 2 & 3: Decision boundaries (training)\n",
    "plot_decision_boundary(axes[0, 1], model_shallow, X_train, y_train, 'Shallow - Training')\n",
    "plot_decision_boundary(axes[0, 2], model_deep, X_train, y_train, 'Deep - Training')\n",
    "\n",
    "# Plot 4 & 5: Decision boundaries (test)\n",
    "plot_decision_boundary(axes[1, 1], model_shallow, X_test, y_test, 'Shallow - Test')\n",
    "contour = plot_decision_boundary(axes[1, 2], model_deep, X_test, y_test, 'Deep - Test')\n",
    "\n",
    "# Plot 6: Accuracy comparison\n",
    "model_names = ['Shallow\\n(1 layer)', 'Deep\\n(2 layers)']\n",
    "train_accs = [\n",
    "    np.mean(model_shallow.predict(X_train) == y_train),\n",
    "    np.mean(model_deep.predict(X_train) == y_train)\n",
    "]\n",
    "test_accs = [\n",
    "    np.mean(model_shallow.predict(X_test) == y_test),\n",
    "    np.mean(model_deep.predict(X_test) == y_test)\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x_pos - width/2, train_accs, width, label='Train', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Model', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(model_names)\n",
    "axes[1, 0].set_ylim(0.7, 1.0)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (train, test) in enumerate(zip(train_accs, test_accs)):\n",
    "    axes[1, 0].text(i - width/2, train + 0.01, f'{train:.3f}', ha='center', fontsize=10)\n",
    "    axes[1, 0].text(i + width/2, test + 0.01, f'{test:.3f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Layer Activations\n",
    "\n",
    "Let's see what the hidden layers learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations from deep model\n",
    "_ = model_deep.forward(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Input layer (original features)\n",
    "im0 = axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                      cmap='RdYlBu_r', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Input Layer (Original Features)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im0, ax=axes[0], label='Class')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Hidden layer 1 activations (first 2 dimensions)\n",
    "h1_activations = model_deep.a_cache[1]  # After first hidden layer\n",
    "im1 = axes[1].scatter(h1_activations[:, 0], h1_activations[:, 1], c=y_train,\n",
    "                      cmap='RdYlBu_r', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Neuron 1 Activation', fontsize=12)\n",
    "axes[1].set_ylabel('Neuron 2 Activation', fontsize=12)\n",
    "axes[1].set_title('Hidden Layer 1 (First 2 Neurons)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[1], label='Class')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Hidden layer 2 activations (first 2 dimensions)\n",
    "h2_activations = model_deep.a_cache[2]  # After second hidden layer\n",
    "im2 = axes[2].scatter(h2_activations[:, 0], h2_activations[:, 1], c=y_train,\n",
    "                      cmap='RdYlBu_r', s=30, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "axes[2].set_xlabel('Neuron 1 Activation', fontsize=12)\n",
    "axes[2].set_ylabel('Neuron 2 Activation', fontsize=12)\n",
    "axes[2].set_title('Hidden Layer 2 (First 2 Neurons)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[2], label='Class')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how the layers progressively separate the classes!\")\n",
    "print(\"Input → Hidden1: Features get transformed\")\n",
    "print(\"Hidden1 → Hidden2: Classes become more separable\")\n",
    "print(\"Hidden2 → Output: Simple linear separation possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experiment with Even Deeper Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different architectures\n",
    "architectures = {\n",
    "    'Shallow (1 layer)': [2, 20, 1],\n",
    "    'Medium (2 layers)': [2, 16, 8, 1],\n",
    "    'Deep (3 layers)': [2, 16, 12, 8, 1],\n",
    "    'Very Deep (4 layers)': [2, 20, 16, 12, 8, 1]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, arch in architectures.items():\n",
    "    print(f\"\\nTraining {name}: {arch}\")\n",
    "    model = MLP_Deep(\n",
    "        layer_sizes=arch,\n",
    "        learning_rate=0.3,\n",
    "        n_iterations=2000,\n",
    "        activation='relu'\n",
    "    )\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    train_acc = np.mean(model.predict(X_train) == y_train)\n",
    "    test_acc = np.mean(model.predict(X_test) == y_test)\n",
    "    total_params = sum(w.size + b.size for w, b in zip(model.weights, model.biases))\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'params': total_params\n",
    "    }\n",
    "    \n",
    "    print(f\"  Parameters: {total_params}\")\n",
    "    print(f\"  Train Acc:  {train_acc:.4f}\")\n",
    "    print(f\"  Test Acc:   {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all architectures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "for name, result in results.items():\n",
    "    axes[0].plot(result['model'].cost_history, label=name, linewidth=2, alpha=0.8)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('Cost', fontsize=12)\n",
    "axes[0].set_title('Learning Curves: Different Depths', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy vs Parameters\n",
    "params_list = [results[name]['params'] for name in architectures.keys()]\n",
    "train_accs = [results[name]['train_acc'] for name in architectures.keys()]\n",
    "test_accs = [results[name]['test_acc'] for name in architectures.keys()]\n",
    "\n",
    "axes[1].plot(params_list, train_accs, 'o-', linewidth=2, markersize=10, label='Train', alpha=0.8)\n",
    "axes[1].plot(params_list, test_accs, 's-', linewidth=2, markersize=10, label='Test', alpha=0.8)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(architectures.keys()):\n",
    "    axes[1].annotate(name.split()[0], \n",
    "                     (params_list[i], test_accs[i]),\n",
    "                     textcoords=\"offset points\", \n",
    "                     xytext=(0,10), \n",
    "                     ha='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "axes[1].set_xlabel('Number of Parameters', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy vs Model Complexity', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.7, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n### What We Learned:\n\n1. **Deep Learning = Multiple Hidden Layers**\n   - Shallow: Input → Hidden → Output\n   - Deep: Input → Hidden₁ → Hidden₂ → ... → Output\n   - Each layer transforms features progressively\n\n2. **Backpropagation Scales Beautifully**\n   - Same pattern for each layer:\n     1. Compute error: $\\frac{\\partial L}{\\partial z} = (\\text{error from next layer}) \\odot \\text{activation}'(z)$\n     2. Weight gradient: $\\frac{\\partial L}{\\partial W} = \\text{input}^T \\times \\text{error}$\n     3. Bias gradient: $\\frac{\\partial L}{\\partial b} = \\sum(\\text{error})$\n     4. Propagate back: error for prev layer = error $\\times W^T$\n   - Works for ANY number of layers!\n\n3. **Hierarchical Feature Learning**\n   - **Layer 1**: Simple features\n   - **Layer 2**: Combinations of simple features\n   - **Layer 3**: High-level abstract features\n   - **Output**: Final decision\n\n4. **The Backpropagation Recipe (for any layer $i$):**\n   ```python\n   # Forward pass (save these!):\n   z[i] = a[i-1] @ W[i] + b[i]\n   a[i] = activation(z[i])\n   \n   # Backward pass:\n   dz[i] = da[i] * activation'(z[i])  # Error at this layer\n   dW[i] = a[i-1].T @ dz[i]           # Weight gradient\n   db[i] = sum(dz[i])                 # Bias gradient\n   da[i-1] = dz[i] @ W[i].T          # Propagate to previous layer\n   ```\n\n5. **Depth vs Width Trade-off**\n   - **Wider networks** (more neurons per layer): More parameters\n   - **Deeper networks** (more layers): Better feature hierarchy\n   - Often: Deeper networks learn better with fewer parameters\n\n6. **Practical Insights:**\n   - **More layers ≠ always better**\n     - Need more data for very deep networks\n     - Risk of overfitting\n     - Vanishing gradients in very deep networks\n   \n   - **Start shallow, go deeper if needed**\n     - Try 1 hidden layer first\n     - Add layers if performance plateaus\n     - Monitor train vs test accuracy\n   \n   - **Activation functions matter**\n     - ReLU: Most common for hidden layers\n     - Sigmoid: Good for binary output\n     - Tanh: Sometimes better than sigmoid for hidden layers\n\n### Mathematical Beauty:\n\n**Forward Pass (layer $i$):**\n$$z^{(i)} = a^{(i-1)}W^{(i)} + b^{(i)}$$\n$$a^{(i)} = g(z^{(i)})$$\n\n**Backward Pass (layer $i$):**\n$$\\delta^{(i)} = (\\delta^{(i+1)}W^{(i+1)^T}) \\odot g'(z^{(i)})$$\n$$\\frac{\\partial L}{\\partial W^{(i)}} = \\frac{1}{m}a^{(i-1)^T}\\delta^{(i)}$$\n$$\\frac{\\partial L}{\\partial b^{(i)}} = \\frac{1}{m}\\sum\\delta^{(i)}$$\n\nWhere:\n- $\\delta^{(i)}$ = error at layer $i$\n- $g$ = activation function\n- $g'$ = derivative of activation\n- $\\odot$ = element-wise multiplication\n\n**This is the universal pattern for training neural networks!**\n\n### Why This Matters:\n\nYou now understand:\n1. ✅ Linear Regression (simple prediction)\n2. ✅ Logistic Regression (binary classification)\n3. ✅ Shallow Neural Networks (1 hidden layer)\n4. ✅ Deep Neural Networks (multiple hidden layers)\n\n**You understand the fundamentals of deep learning!**\n\nEverything else (CNNs, RNNs, Transformers, etc.) builds on these same principles:\n- Forward pass: compute predictions\n- Loss function: measure error\n- Backpropagation: compute gradients\n- Gradient descent: update parameters\n\nThe only differences are:\n- Architecture (how layers are connected)\n- Activation functions\n- Loss functions\n- Optimization tricks\n\n**But the core idea is always the same: gradient descent with backpropagation!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}